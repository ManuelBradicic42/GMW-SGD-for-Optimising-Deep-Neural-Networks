{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67c9229",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce69c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/research/TopDownVideo/mb01761/conda/.conda/envs/torch-kernel/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import datetime\n",
    "import operator\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from deap import creator, base, tools, algorithms, benchmarks\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3326b",
   "metadata": {},
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d52e3cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_loader(data_dir, \n",
    "                           batch_size, \n",
    "                           random_seed,\n",
    "                           shuffle_dataset,\n",
    "                           validation_split,\n",
    "                           tf):\n",
    "\n",
    "    train_ = torchvision.datasets.CIFAR10(root=data_dir, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "    #Creating data indices for training and validation splits:\n",
    "    train_dataset_size = len(train_)\n",
    "    indices = list(range(train_dataset_size))\n",
    "    split = int(np.floor(validation_split * train_dataset_size))\n",
    "\n",
    "    if shuffle_dataset:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_indices,val_indices = indices[split:], indices[:split]\n",
    "\n",
    "\n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_, batch_size=batch_size, \n",
    "                                               sampler=train_sampler)\n",
    "    validation_loader = torch.utils.data.DataLoader(train_, batch_size=batch_size,\n",
    "                                                    sampler=valid_sampler)\n",
    "    \n",
    "    return (train_loader, validation_loader)\n",
    "\n",
    "\n",
    "\n",
    "def get_train_lodader(data_dir,\n",
    "                   batch_size,\n",
    "                   random_seed,\n",
    "                   shuffle_dataset,\n",
    "                   transform):\n",
    "\n",
    "    train_ = torchvision.datasets.CIFAR10(root=data_dir, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_, batch_size=batch_size)\n",
    "\n",
    "    return(train_loader)\n",
    "\n",
    "def get_test_lodader(data_dir,\n",
    "                   batch_size,\n",
    "                   random_seed,\n",
    "                   shuffle_dataset,\n",
    "                   transform):\n",
    "\n",
    "    test_ = torchvision.datasets.CIFAR10(root=data_dir, train=False,\n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_, batch_size=batch_size)\n",
    "\n",
    "    return(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90788e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed= 1\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a9bea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "validation_split = 0\n",
    "shuffle_dataset = True\n",
    "\n",
    "batch_size = 32\n",
    "data_dir = './data'\n",
    "\n",
    "### Classes from CIFAR10\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(32),transforms.ToTensor(),\n",
    "                                    transforms.Normalize(\n",
    "                                        mean=[0.485, 0.456, 0.406], \n",
    "                                        std=[0.229, 0.224, 0.225])])\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#train_loader, validation_loader = get_train_valid_loader('./data', batch_size, random_seed, shuffle_dataset, validation_split, transform)\n",
    "\n",
    "\n",
    "train_loader = get_train_lodader('./data', batch_size, random_seed, shuffle_dataset, transform)\n",
    "test_loader = get_test_lodader('./data', batch_size, random_seed, shuffle_dataset, transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c321220",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd947aa",
   "metadata": {},
   "source": [
    "### Initialisation and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d639b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squared_diff(actual, predicted):\n",
    "    sum_ = 0\n",
    "    for yi, yi_hat in zip(actual, predicted):\n",
    "        sum_ += ((yi-yi_hat)**2)\n",
    "    \n",
    "    return sum_\n",
    "\n",
    "def mse(s, n):\n",
    "    return (s/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5864323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training function\n",
    "### https://debuggercafe.com/transfer-learning-using-efficientnet-pytorch/\n",
    "def train(model, trainloader, optimizer, criterion):    \n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    print(len(trainloader))\n",
    "    \n",
    "    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "\n",
    "        count += 1\n",
    "        image, labels = data\n",
    "        \n",
    "        image = image.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass.\n",
    "        outputs = model(image)\n",
    "        \n",
    "        # Calculate the loss.\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the accuracy.\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_correct += (preds == labels).sum().item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights.\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    epoch_loss = running_loss / count\n",
    "    epoch_acc = 100. * (running_correct / len(trainloader.dataset))\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, testloader, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    \n",
    "    count = 0\n",
    "    sum_ = 0\n",
    "    with torch.no_grad():\n",
    "        #for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n",
    "        for i, data in enumerate(testloader):\n",
    "            count += 1\n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass.\n",
    "            outputs = model(image)\n",
    "            \n",
    "            # Calculate the loss.\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate the accuracy.\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            \n",
    "            sum_ += sum_squared_diff(labels,preds)\n",
    "        \n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    epoch_loss = running_loss / count\n",
    "    epoch_acc = 100. * (running_correct / len(testloader.dataset))\n",
    "    # MSE for validation\n",
    "    MSE = mse(sum_, len(test_loader.dataset))\n",
    "    \n",
    "    return epoch_loss, epoch_acc, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54b3c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 8, 5)\n",
    "        self.fc1 = nn.Linear(200, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3998afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epochs, model, optimizer, criterion, pretrained, time, val):\n",
    "    torch.save({\n",
    "                'epoch': epochs,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                }, f\"./model_efficientNetB0_{epochs}_{val}_{time}.pth\")    \n",
    "\n",
    "\n",
    "## Load saved model\n",
    "def load_model(name):\n",
    "    model = torch.load(name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(num_classes, pre_trained=True, fine_tune=True, saved=None):\n",
    "#     model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "\n",
    "    model = MyEfficientNet()\n",
    "    \n",
    "    if fine_tune:\n",
    "        print('[INFO]: Fine-tuning all layers...')\n",
    "        for params in model.parameters():\n",
    "            params.requires_grad = True\n",
    "            \n",
    "    elif not fine_tune:\n",
    "        print('[INFO]: Freezing hidden layers...')\n",
    "        for name, param in list(model.named_parameters())[:-2]:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        for name, param in list(model.named_parameters())[-2:]:\n",
    "            param.requires_grad = True\n",
    "        \n",
    "#         for params in model.parameters():\n",
    "#             params.requires_grad = False\n",
    "    \n",
    "#     model.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes) \n",
    "    \n",
    "    \n",
    "    if saved:\n",
    "        loaded_dict = load_model(saved)\n",
    "        model.load_state_dict(loaded_dict['model_state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "361ce806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation device: cuda\n",
      "Learning rate: 0.001\n",
      "Number of epochs: 20\n"
     ]
    }
   ],
   "source": [
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "\n",
    "LOADED = False\n",
    "load_model_name = \"model_efficientNetB0_5_2022-11-22_16-50.pth\"\n",
    "\n",
    "# Loads model weights\n",
    "pretrained = False\n",
    "# False -> freezes all the layers but the last one; True -> unfreezes all the layers\n",
    "fine_tune = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Computation device: {device}\")\n",
    "\n",
    "lr = 1e-3\n",
    "print(f\"Learning rate: {lr}\")\n",
    "\n",
    "epochs = 20\n",
    "print(f\"Number of epochs: {epochs}\")\n",
    "\n",
    "# Save every x epochs\n",
    "save_epoch = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa36f2",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c53b6541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=200, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model\n",
    "model = Net()\n",
    "# Moving the model to the device (GPU)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3d3b23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer.\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "# Loss function.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode = 'min',patience=3, cooldown=2, factor=0.1)\n",
    "# Model saving interval\n",
    "save_interval = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b87f2e8",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1d49120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 20\n",
      "LR:{0.001}\n",
      "1563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1563/1563 [00:13<00:00, 112.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.104, training acc: 21.678\n",
      "Validation loss: 1.791, validation acc: 34.360, mse:13.003\n",
      "--------------------------------------------------\n",
      "Save Model\n",
      "[INFO]: Epoch 2 of 20\n",
      "LR:{0.001}\n",
      "1563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1563/1563 [00:14<00:00, 111.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.644, training acc: 39.766\n",
      "Validation loss: 1.535, validation acc: 43.420, mse:11.526\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 3 of 20\n",
      "LR:{0.001}\n",
      "1563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1563/1563 [00:13<00:00, 113.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.491, training acc: 45.746\n",
      "Validation loss: 1.441, validation acc: 47.410, mse:10.696\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 4 of 20\n",
      "LR:{0.001}\n",
      "1563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1563/1563 [00:14<00:00, 110.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.404, training acc: 49.584\n",
      "Validation loss: 1.368, validation acc: 50.620, mse:9.837\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 5 of 20\n",
      "LR:{0.001}\n",
      "1563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▋                                    | 107/1563 [00:00<00:13, 108.32it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO]: Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR:\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 13\u001b[0m train_epoch_loss, train_epoch_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m valid_epoch_loss, valid_epoch_acc, valid_epoch_mse \u001b[38;5;241m=\u001b[39m validate(model, test_loader, criterion)\n\u001b[1;32m     16\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(valid_epoch_mse)  \n",
      "Cell \u001b[0;32mIn [6], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     32\u001b[0m running_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Update the weights.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/vol/research/TopDownVideo/mb01761/conda/.conda/envs/torch-kernel/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/research/TopDownVideo/mb01761/conda/.conda/envs/torch-kernel/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "if not LOADED:\n",
    "  # Lists to keep track of losses and accuracies.\n",
    "  train_loss, valid_loss = [], []\n",
    "  train_acc, valid_acc = [], []\n",
    "  valid_mse = []\n",
    "\n",
    "  # Start the training.\n",
    "  for epoch in range(epochs):\n",
    "      print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n",
    "      print(\"LR:{%s}\" %optimizer.param_groups[0]['lr'])\n",
    "      train_epoch_loss, train_epoch_acc = train(model, train_loader, optimizer, criterion)\n",
    "      valid_epoch_loss, valid_epoch_acc, valid_epoch_mse = validate(model, test_loader, criterion)\n",
    "      \n",
    "      scheduler.step(valid_epoch_mse)  \n",
    "        \n",
    "      train_loss.append(train_epoch_loss)\n",
    "      valid_loss.append(valid_epoch_loss)\n",
    "      train_acc.append(train_epoch_acc)\n",
    "      valid_acc.append(valid_epoch_acc)\n",
    "      valid_mse.append(valid_epoch_mse)\n",
    "      print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n",
    "      print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}, mse:{valid_epoch_mse:.3f}\")\n",
    "      print('-'*50)\n",
    "\n",
    "      if epoch % save_interval == 0:\n",
    "        print(\"Save Model\")\n",
    "        save_model(epochs, model, optimizer, criterion, pretrained, time, valid_epoch_acc)\n",
    "    \n",
    "  # Save the trained model weights.\n",
    "  save_model(epochs, model, optimizer, criterion, pretrained, time, valid_acc[-1])\n",
    "  print('TRAINING COMPLETE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af0f75",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d615b4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.3087721465113824, 11.31, tensor(13.5055, device='cuda:0'))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aca2e9",
   "metadata": {},
   "source": [
    "## Adam-SLPso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f25dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam_SLPSO():\n",
    "    def __init__(self,dimension):     \n",
    "        self.populationSize  = 5\n",
    "        self.iterations      = 3\n",
    "\n",
    "        # Parameter Setup - PSO\n",
    "        self.posMinInit      = - 1\n",
    "        self.posMaxInit      = + 1\n",
    "        self.VMaxInit        = 1.5\n",
    "        self.VMinInit        = 0.5\n",
    "        self.interval        = 10\n",
    "        self.dimension       = dimension\n",
    "\n",
    "        self.alpha = 0.5\n",
    "        self.beta = 1e-3\n",
    "        self.epsilon = self.beta * (self.dimension/self.populationSize)\n",
    "        self.cons_m = self.populationSize + math.floor(self.dimension/10)\n",
    "        self.best_losses = []\n",
    "\n",
    "        self.c1 = 2\n",
    "        self.c2 = 2\n",
    "\n",
    "        # Deap Initialisation\n",
    "        self.toolbox = base.Toolbox()\n",
    "        \n",
    "        self.toolbox.register(\"update\", self.updateParticleSLPSO)\n",
    "        self.toolbox.register(\"particle\", self.generate, size=dimension, smin=self.posMinInit, smax=self.posMaxInit)\n",
    "        self.toolbox.register(\"population\", tools.initRepeat, list, self.toolbox.particle)\n",
    "        creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,)) # -1 is for minimise\n",
    "        creator.create(\"Particle\", list, fitness=creator.FitnessMin, speed=list, smin=None, smax=None, best=None)\n",
    "\n",
    "        self.toolbox.register(\"evaluate\", self.evalNet) \n",
    "\n",
    "        self.pop = self.toolbox.population(n=self.populationSize) \n",
    "        \n",
    "        self.best = None\n",
    "\n",
    "        self.stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "        self.stats.register(\"avg\", np.mean)\n",
    "        self.stats.register(\"std\", np.std)\n",
    "        self.stats.register(\"min\", np.min)\n",
    "        self.stats.register(\"max\", np.max)\n",
    "\n",
    "        self.logbook = tools.Logbook()\n",
    "        self.logbook.header = [\"gen\", \"evals\"] + self.stats.fields\n",
    "\n",
    "    \n",
    "    def processing(self,generations, patience):\n",
    "        gen = 0\n",
    "        improving = True\n",
    "        counter_stagnation = 0\n",
    "        while(improving and gen<generations):\n",
    "            print(\"   -- Generation %i --\" % gen)\n",
    "            gen_mse = []\n",
    "            \n",
    "            #Evaluation of particles\n",
    "            for part in self.pop:\n",
    "                fitness = self.toolbox.evaluate(part)\n",
    "                part.fitness.values = fitness\n",
    "                gen_mse.append(fitness)\n",
    "\n",
    "            #Setting the 0th old_gbest\n",
    "            if(gen==0):\n",
    "                old_gbest = self.pop[0].fitness.values\n",
    "                \n",
    "            # Saving gbest\n",
    "            self.best_ind = tools.selBest(self.pop, 1)[0]\n",
    "\n",
    "            #Sorting particles by fitness\n",
    "            self.pop.sort(key=lambda x: x.fitness, reverse=True)\n",
    "            mu = self.calculate_mu(self.pop)\n",
    "            \n",
    "            for i in range(len(self.pop)):\n",
    "                print(self.pop[i].fitness.values)\n",
    "    \n",
    "\n",
    "            #for i  in reversed(range(len(self.pop)-1)):  # start with worst particle, and go in reverse towards best\n",
    "                                                          # don't do element 0 (best). Hence the i+1 below.\n",
    "            for i  in range(len(self.pop)-1):\n",
    "                if random.uniform(0, 1) < self.calculate_l_prob(i): # learning probability for that particle\n",
    "                          self.toolbox.update(self.pop[i], self.pop, mu, i)\n",
    "            \n",
    "            ### Checking if the fitness improves \n",
    "            if (old_gbest <= self.best_ind.fitness.values):\n",
    "                print(\"TRUE\")\n",
    "                counter_stagnation +=1\n",
    "                if(counter_stagnation == patience):\n",
    "                    improving = False\n",
    "            else:\n",
    "                print(\"FALSE\")\n",
    "                counter_stagnation = 0\n",
    "                old_gbest = self.best_ind.fitness.values\n",
    "\n",
    "\n",
    "            print(counter_stagnation)\n",
    "            print(old_gbest)\n",
    "            print(self.best_ind.fitness.values)\n",
    "            \n",
    "            self.best_ind = tools.selBest(self.pop, 1)[0]\n",
    "\n",
    "            gen +=1\n",
    "            \n",
    "            # Saving the best individual in the generation\n",
    "            best_loss = min(gen_mse)\n",
    "            print(f\"*** BEST loss in generation {gen} = {best_loss}\")\n",
    "            self.best_losses.append(best_loss)\n",
    "\n",
    "        \n",
    "    \n",
    "   \n",
    "    # ************************************************\n",
    "    # calculate_mu(pop) :\n",
    "    # Calculating mu values for particle update\n",
    "    # INPUT: \n",
    "    #   list  -    pop      - List of populations\n",
    "    # \n",
    "    # OUTPUT : \n",
    "    #   list  -    mu_list  - List of mu values \n",
    "    # ************************************************\n",
    "    def calculate_mu(self, population):\n",
    "        mu_list = list()\n",
    "        for i in range(self.dimension):\n",
    "            temp = 0\n",
    "            for j in population:\n",
    "                temp += j[i]\n",
    "            temp /= self.populationSize\n",
    "            mu_list.append(temp)\n",
    "        return mu_list\n",
    "\n",
    "    # ************************************************\n",
    "    # calculate_l_prob(idx) :\n",
    "    # Calculating learning probability to assess if a particle\n",
    "    # needs to be updated\n",
    "    # INPUT: \n",
    "    #   int  -    idx  -    index i of population\n",
    "    # \n",
    "    # OUTPUT : \n",
    "    #   int  -    l_p  -    learning probability return\n",
    "    # ************************************************\n",
    "    def calculate_l_prob(self, idx):\n",
    "        l_p = (1-((idx-1)/self.cons_m))**(self.alpha*math.log(math.sqrt(math.ceil(self.dimension/self.populationSize))))   \n",
    "        return l_p\n",
    "\n",
    "    \n",
    "        \n",
    "    # ************************************************\n",
    "    # updateParticleSLPSO(part, pop, mu, i) :\n",
    "    # calculating new values for particles in population \n",
    "    #\n",
    "    # INPUT: \n",
    "    #   obj  -    part  -    single unit of SL-PSO algorithm\n",
    "    #   obj  -    pop  -    population\n",
    "    #   list  -    mu  -   list of calculated mu\n",
    "    # \n",
    "    # OUTPUT : \n",
    "    #   int  -    MSE  -    Mean Square Error\n",
    "    # ************************************************\n",
    "    def updateParticleSLPSO(self, part, pop, mu, i):\n",
    "        demonstrator=random.choice(list(pop[i:len(pop)]))\n",
    "\n",
    "        r1 = [random.uniform(0, 1)*0.05]*dimension\n",
    "        r2 = [random.uniform(0, 1)*0.9]*dimension\n",
    "        r3 = [random.uniform(0, 1)*0.9]*dimension\n",
    "\n",
    "        v_r0 = [x for x in map(operator.mul, r1, part.speed)]\n",
    "        v_r1 = [x for x in map(operator.mul, r2, map(operator.sub, demonstrator, part))] # learning from the demonstrator\n",
    "        v_r2 = [self.epsilon*x for x in map(operator.mul, r3, map(operator.sub, mu, part))] # social learning from the mean\n",
    "        part.speed = [x for x in map(operator.add, v_r0, map(operator.add, v_r1, v_r2))]\n",
    "       \n",
    "        part[:] = list(map(operator.add, part, part.speed))\n",
    "    \n",
    "    # ************************************************\n",
    "    # evalNet(individual) :\n",
    "    # Calculating the accuracy of the model, in this case on CIFAR-100\n",
    "    # using the network weights calculated by an individual \n",
    "    #\n",
    "    # INPUT: \n",
    "    #   obj  -    individual  -    single unit of SL-PSO algorithm\n",
    "    # \n",
    "    # OUTPUT : \n",
    "    #   int  -    MSE  -    Mean Square Error\n",
    "    # ************************************************\n",
    "    def generate(self, size, smin, smax):\n",
    "        part = creator.Particle(random.uniform(self.posMinInit, self.posMaxInit) for _ in range(size)) \n",
    "        part.speed = [random.uniform(self.VMinInit, self.VMaxInit) for _ in range(size)]\n",
    "        part.smin = smin #speed clamping values\n",
    "        part.smax = smax\n",
    "        return part\n",
    "\n",
    "    # ************************************************\n",
    "    # evalNet(individual) :\n",
    "    # Calculating the accuracy of the model, in this case on CIFAR-100\n",
    "    # using the network weights calculated by an individual \n",
    "    #\n",
    "    # INPUT: \n",
    "    #   obj  -    individual  -    single unit of SL-PSO algorithm\n",
    "    # \n",
    "    # OUTPUT : \n",
    "    #   int  -    MSE  -    Mean Square Error\n",
    "    # ************************************************\n",
    "    def evalNet(self, individual):\n",
    "        # Updating the network with individual's decision variable\n",
    "        self.updateWeights(individual)\n",
    "\n",
    "\n",
    "        loss, accuracy, MSE = validate(model, test_loader, criterion)        \n",
    "        print(f\"Accuracy : {accuracy}; Loss: {loss}; MSE: {MSE}\")\n",
    "        return (MSE,)\n",
    "    \n",
    "    # ************************************************\n",
    "    # updateWeights(individual) :\n",
    "    # Updating the weights of a NN using the predicted dimensions\n",
    "    # of an individual\n",
    "    #\n",
    "    # INPUT: \n",
    "    #   obj  -    individual  -    single unit of SL-PSO algorithm\n",
    "    # \n",
    "    def updateWeights(self, individual):\n",
    "        # Getting the state dictionary which includes weights and biases\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Counter\n",
    "        prev = 0\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "          if param.requires_grad:\n",
    "            if ('weight' or 'bias' in name):\n",
    "              # Getting the shape of the current weight/bias\n",
    "              shape = param.shape\n",
    "\n",
    "              # Getting the number of variables that make up the parameter\n",
    "              num_var = len(param.flatten())\n",
    "\n",
    "              # Getting a subset of the particle's decision variables and shaping it\n",
    "              # to fit the network\n",
    "              new_parameter = np.reshape(individual[prev:prev+num_var], shape)\n",
    "\n",
    "              # Updating the state variable\n",
    "              state_dict[name] = torch.Tensor(new_parameter)\n",
    "              prev += num_var\n",
    "\n",
    "        # Updating the network with the weights\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79440c4f",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ea530302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 16938\n"
     ]
    }
   ],
   "source": [
    "dimension = 0 \n",
    "for name, param in model.named_parameters():\n",
    "  if param.requires_grad:\n",
    "    if ('weight' or 'bias' in name):\n",
    "      dimension += len(param.flatten())\n",
    "\n",
    "print(f\"Total number of parameters: {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196b3de0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** EVOLUTION 0 **\n",
      "   -- Generation 0 --\n",
      "Accuracy : 10.81; Loss: 1680.8312060078874; MSE: 15.049699783325195\n",
      "Accuracy : 10.92; Loss: 2043.292699649311; MSE: 14.060599327087402\n",
      "Accuracy : 10.86; Loss: 2673.1448635308507; MSE: 18.61629867553711\n",
      "Accuracy : 9.41; Loss: 2533.1805443321937; MSE: 19.15989875793457\n",
      "Accuracy : 8.67; Loss: 3736.907242942542; MSE: 16.564899444580078\n",
      "(tensor(14.0606, device='cuda:0'),)\n",
      "(tensor(15.0497, device='cuda:0'),)\n",
      "(tensor(16.5649, device='cuda:0'),)\n",
      "(tensor(18.6163, device='cuda:0'),)\n",
      "(tensor(19.1599, device='cuda:0'),)\n",
      "FALSE\n",
      "0\n",
      "(tensor(14.0606, device='cuda:0'),)\n",
      "(tensor(14.0606, device='cuda:0'),)\n",
      "*** BEST loss in generation 1 = (tensor(14.0606, device='cuda:0'),)\n",
      "   -- Generation 1 --\n",
      "Accuracy : 9.370000000000001; Loss: 1350.8945606950754; MSE: 19.85610008239746\n",
      "Accuracy : 8.7; Loss: 558798.0472244408; MSE: 15.16469955444336\n",
      "Accuracy : 9.26; Loss: 5466.819074636831; MSE: 10.151299476623535\n",
      "Accuracy : 10.96; Loss: 146871.72885882587; MSE: 18.6658992767334\n",
      "Accuracy : 9.41; Loss: 2533.1805443321937; MSE: 19.15989875793457\n",
      "(tensor(10.1513, device='cuda:0'),)\n",
      "(tensor(15.1647, device='cuda:0'),)\n",
      "(tensor(18.6659, device='cuda:0'),)\n",
      "(tensor(19.1599, device='cuda:0'),)\n",
      "(tensor(19.8561, device='cuda:0'),)\n",
      "FALSE\n",
      "0\n",
      "(tensor(10.1513, device='cuda:0'),)\n",
      "(tensor(10.1513, device='cuda:0'),)\n",
      "*** BEST loss in generation 2 = (tensor(10.1513, device='cuda:0'),)\n",
      "   -- Generation 2 --\n",
      "Accuracy : 9.73; Loss: 387490.8789936102; MSE: 15.05519962310791\n",
      "Accuracy : 8.97; Loss: 320779.0787240415; MSE: 12.39109992980957\n",
      "Accuracy : 11.39; Loss: 83148.9188298722; MSE: 18.510299682617188\n",
      "Accuracy : 10.73; Loss: 490.07010102195864; MSE: 19.295299530029297\n",
      "Accuracy : 9.370000000000001; Loss: 1350.8945606950754; MSE: 19.85610008239746\n",
      "(tensor(12.3911, device='cuda:0'),)\n",
      "(tensor(15.0552, device='cuda:0'),)\n",
      "(tensor(18.5103, device='cuda:0'),)\n",
      "(tensor(19.2953, device='cuda:0'),)\n",
      "(tensor(19.8561, device='cuda:0'),)\n",
      "TRUE\n",
      "1\n",
      "(tensor(10.1513, device='cuda:0'),)\n",
      "(tensor(12.3911, device='cuda:0'),)\n",
      "*** BEST loss in generation 3 = (tensor(12.3911, device='cuda:0'),)\n",
      "   -- Generation 3 --\n",
      "Accuracy : 9.41; Loss: 546137.1175119808; MSE: 15.818099975585938\n",
      "Accuracy : 9.700000000000001; Loss: 25805.445549620606; MSE: 17.065898895263672\n",
      "Accuracy : 11.0; Loss: 25680.769100688896; MSE: 14.52199935913086\n",
      "Accuracy : 9.08; Loss: 617.2757126683244; MSE: 15.627599716186523\n",
      "Accuracy : 9.370000000000001; Loss: 1350.8945606950754; MSE: 19.85610008239746\n",
      "(tensor(14.5220, device='cuda:0'),)\n",
      "(tensor(15.6276, device='cuda:0'),)\n",
      "(tensor(15.8181, device='cuda:0'),)\n",
      "(tensor(17.0659, device='cuda:0'),)\n",
      "(tensor(19.8561, device='cuda:0'),)\n",
      "TRUE\n",
      "2\n",
      "(tensor(10.1513, device='cuda:0'),)\n",
      "(tensor(14.5220, device='cuda:0'),)\n",
      "*** BEST loss in generation 4 = (tensor(14.5220, device='cuda:0'),)\n",
      "   -- Generation 4 --\n",
      "Accuracy : 9.51; Loss: 2242.619238515251; MSE: 17.854700088500977\n",
      "Accuracy : 9.06; Loss: 926.1553093175919; MSE: 16.392000198364258\n",
      "Accuracy : 9.71; Loss: 28409.50780001997; MSE: 17.937198638916016\n",
      "Accuracy : 9.54; Loss: 3400.9930540822184; MSE: 17.334699630737305\n",
      "Accuracy : 9.370000000000001; Loss: 1350.8945606950754; MSE: 19.85610008239746\n",
      "(tensor(16.3920, device='cuda:0'),)\n",
      "(tensor(17.3347, device='cuda:0'),)\n",
      "(tensor(17.8547, device='cuda:0'),)\n",
      "(tensor(17.9372, device='cuda:0'),)\n",
      "(tensor(19.8561, device='cuda:0'),)\n",
      "TRUE\n",
      "3\n",
      "(tensor(10.1513, device='cuda:0'),)\n",
      "(tensor(16.3920, device='cuda:0'),)\n",
      "*** BEST loss in generation 5 = (tensor(16.3920, device='cuda:0'),)\n",
      "   -- Generation 5 --\n",
      "Accuracy : 10.2; Loss: 988.603259394344; MSE: 14.497900009155273\n",
      "Accuracy : 9.33; Loss: 502.9546891965043; MSE: 17.02829933166504\n",
      "Accuracy : 12.049999999999999; Loss: 368863.0744808307; MSE: 16.09149932861328\n",
      "Accuracy : 9.39; Loss: 545.3148019808931; MSE: 24.422399520874023\n",
      "Accuracy : 9.370000000000001; Loss: 1350.8945606950754; MSE: 19.85610008239746\n",
      "(tensor(14.4979, device='cuda:0'),)\n",
      "(tensor(16.0915, device='cuda:0'),)\n",
      "(tensor(17.0283, device='cuda:0'),)\n",
      "(tensor(19.8561, device='cuda:0'),)\n",
      "(tensor(24.4224, device='cuda:0'),)\n",
      "TRUE\n",
      "4\n",
      "(tensor(10.1513, device='cuda:0'),)\n",
      "(tensor(14.4979, device='cuda:0'),)\n",
      "*** BEST loss in generation 6 = (tensor(14.4979, device='cuda:0'),)\n",
      "** EVOLUTION 1 **\n",
      "   -- Generation 0 --\n",
      "Accuracy : 12.02; Loss: 679042.3374600639; MSE: 17.137100219726562\n",
      "Accuracy : 10.52; Loss: 38970.05699630591; MSE: 19.54399871826172\n",
      "Accuracy : 11.55; Loss: 38921.504686251996; MSE: 19.132699966430664\n",
      "Accuracy : 7.03; Loss: 2021.764934308232; MSE: 20.045499801635742\n",
      "Accuracy : 9.39; Loss: 545.3148019808931; MSE: 24.422399520874023\n",
      "(tensor(17.1371, device='cuda:0'),)\n",
      "(tensor(19.1327, device='cuda:0'),)\n",
      "(tensor(19.5440, device='cuda:0'),)\n",
      "(tensor(20.0455, device='cuda:0'),)\n",
      "(tensor(24.4224, device='cuda:0'),)\n",
      "TRUE\n",
      "1\n",
      "(tensor(17.1371, device='cuda:0'),)\n",
      "(tensor(17.1371, device='cuda:0'),)\n",
      "*** BEST loss in generation 1 = (tensor(17.1371, device='cuda:0'),)\n",
      "   -- Generation 1 --\n",
      "Accuracy : 8.690000000000001; Loss: 7075.270868173423; MSE: 19.622098922729492\n",
      "Accuracy : 11.09; Loss: 25394.910271690296; MSE: 20.023399353027344\n",
      "Accuracy : 11.790000000000001; Loss: 3998.9011441069288; MSE: 19.11240005493164\n",
      "Accuracy : 11.88; Loss: 151796.1215305511; MSE: 19.0487003326416\n",
      "Accuracy : 9.39; Loss: 545.3148019808931; MSE: 24.422399520874023\n",
      "(tensor(19.0487, device='cuda:0'),)\n",
      "(tensor(19.1124, device='cuda:0'),)\n",
      "(tensor(19.6221, device='cuda:0'),)\n",
      "(tensor(20.0234, device='cuda:0'),)\n",
      "(tensor(24.4224, device='cuda:0'),)\n",
      "TRUE\n",
      "2\n",
      "(tensor(17.1371, device='cuda:0'),)\n",
      "(tensor(19.0487, device='cuda:0'),)\n",
      "*** BEST loss in generation 2 = (tensor(19.0487, device='cuda:0'),)\n",
      "   -- Generation 2 --\n",
      "Accuracy : 11.5; Loss: 545946.0012979233; MSE: 18.911800384521484\n",
      "Accuracy : 8.73; Loss: 1553.8546606679313; MSE: 15.626999855041504\n",
      "Accuracy : 9.0; Loss: 2708.006404986397; MSE: 14.378299713134766\n",
      "Accuracy : 9.8; Loss: 557.0334441456171; MSE: 26.417598724365234\n",
      "Accuracy : 9.39; Loss: 545.3148019808931; MSE: 24.422399520874023\n",
      "(tensor(14.3783, device='cuda:0'),)\n",
      "(tensor(15.6270, device='cuda:0'),)\n",
      "(tensor(18.9118, device='cuda:0'),)\n",
      "(tensor(24.4224, device='cuda:0'),)\n",
      "(tensor(26.4176, device='cuda:0'),)\n",
      "FALSE\n",
      "0\n",
      "(tensor(14.3783, device='cuda:0'),)\n",
      "(tensor(14.3783, device='cuda:0'),)\n",
      "*** BEST loss in generation 3 = (tensor(14.3783, device='cuda:0'),)\n",
      "   -- Generation 3 --\n",
      "Accuracy : 10.42; Loss: 1780.6073255447534; MSE: 19.6742000579834\n",
      "Accuracy : 11.35; Loss: 5664.06947633786; MSE: 17.923799514770508\n",
      "Accuracy : 11.469999999999999; Loss: 50268.23782572884; MSE: 19.34429931640625\n",
      "Accuracy : 9.379999999999999; Loss: 666.8591727844823; MSE: 25.119600296020508\n",
      "Accuracy : 9.8; Loss: 557.0334441456171; MSE: 26.417598724365234\n",
      "(tensor(17.9238, device='cuda:0'),)\n",
      "(tensor(19.3443, device='cuda:0'),)\n",
      "(tensor(19.6742, device='cuda:0'),)\n",
      "(tensor(25.1196, device='cuda:0'),)\n",
      "(tensor(26.4176, device='cuda:0'),)\n",
      "TRUE\n",
      "1\n",
      "(tensor(14.3783, device='cuda:0'),)\n",
      "(tensor(17.9238, device='cuda:0'),)\n",
      "*** BEST loss in generation 4 = (tensor(17.9238, device='cuda:0'),)\n",
      "   -- Generation 4 --\n",
      "Accuracy : 8.469999999999999; Loss: 4637.741774098941; MSE: 17.16309928894043\n",
      "Accuracy : 11.16; Loss: 3672575.3426517574; MSE: 19.971899032592773\n",
      "Accuracy : 10.12; Loss: 438.2157006796937; MSE: 21.794599533081055\n",
      "Accuracy : 9.520000000000001; Loss: 598.0582447966067; MSE: 26.030498504638672\n",
      "Accuracy : 9.8; Loss: 557.0334441456171; MSE: 26.417598724365234\n",
      "(tensor(17.1631, device='cuda:0'),)\n",
      "(tensor(19.9719, device='cuda:0'),)\n",
      "(tensor(21.7946, device='cuda:0'),)\n",
      "(tensor(26.0305, device='cuda:0'),)\n",
      "(tensor(26.4176, device='cuda:0'),)\n",
      "TRUE\n",
      "2\n",
      "(tensor(14.3783, device='cuda:0'),)\n",
      "(tensor(17.1631, device='cuda:0'),)\n",
      "*** BEST loss in generation 5 = (tensor(17.1631, device='cuda:0'),)\n",
      "   -- Generation 5 --\n",
      "Accuracy : 10.99; Loss: 5049761.209265175; MSE: 19.978099822998047\n",
      "Accuracy : 12.26; Loss: 55774387.111821085; MSE: 14.471199989318848\n",
      "Accuracy : 11.84; Loss: 239038.09375; MSE: 18.570098876953125\n",
      "Accuracy : 10.290000000000001; Loss: 1594.732448005067; MSE: 20.720998764038086\n",
      "Accuracy : 9.8; Loss: 557.0334441456171; MSE: 26.417598724365234\n",
      "(tensor(14.4712, device='cuda:0'),)\n",
      "(tensor(18.5701, device='cuda:0'),)\n",
      "(tensor(19.9781, device='cuda:0'),)\n",
      "(tensor(20.7210, device='cuda:0'),)\n",
      "(tensor(26.4176, device='cuda:0'),)\n",
      "TRUE\n",
      "3\n",
      "(tensor(14.3783, device='cuda:0'),)\n",
      "(tensor(14.4712, device='cuda:0'),)\n",
      "*** BEST loss in generation 6 = (tensor(14.4712, device='cuda:0'),)\n",
      "   -- Generation 6 --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 10.879999999999999; Loss: 45591740.38338658; MSE: 20.744600296020508\n",
      "Accuracy : 12.1; Loss: 829249.7993210863; MSE: 16.828899383544922\n",
      "Accuracy : 12.280000000000001; Loss: 16891413.134185303; MSE: 15.414999961853027\n"
     ]
    }
   ],
   "source": [
    "evolutions = 10\n",
    "pso = Adam_SLPSO(dimension)\n",
    "\n",
    "for evol in range(evolutions):\n",
    "    print(\"** EVOLUTION %i **\" % evol)\n",
    "\n",
    "    pso.processing(10,4)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
